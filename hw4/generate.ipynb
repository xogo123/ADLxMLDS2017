{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "\n",
    "import skimage\n",
    "import skimage.io\n",
    "import skimage.transform\n",
    "# import matplotlib.pyplot as plt\n",
    "# from matplotlib.pyplot import imshow\n",
    "\n",
    "from PIL import Image\n",
    "import scipy\n",
    "# from sklearn.utils import shuffle\n",
    "\n",
    "from tensorflow.python.framework import ops\n",
    "from tensorflow.python.ops import math_ops\n",
    "def leaky_relu(features, alpha=0.2, name=None):\n",
    "    with ops.name_scope(name, \"LeakyRelu\", [features, alpha]):\n",
    "        features = ops.convert_to_tensor(features, name=\"features\")\n",
    "        alpha = ops.convert_to_tensor(alpha, name=\"alpha\")\n",
    "        return math_ops.maximum(alpha * features, features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## argument setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(1)\n",
    "np.random.seed(1)\n",
    "tf.set_random_seed(1)\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "\n",
    "img_n = 5\n",
    "batch_size = 16\n",
    "d_lda = 10\n",
    "\n",
    "if len(sys.argv) > 1 :\n",
    "    text_path = sys.argv[1]\n",
    "else :\n",
    "    text_path = './data/testing_text.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAN(object):\n",
    "    def __init__(self) :\n",
    "        self.lr = 0.0001\n",
    "        self.lr_pre = 0.0001\n",
    "        self.momentum = 0.5\n",
    "        self.bs = batch_size # batch size is m of paper\n",
    "        self.bs_pre = 128\n",
    "        self.epoch = 10000\n",
    "        self.hair_n = 13\n",
    "        self.eyes_n = 12\n",
    "        self.lda = d_lda\n",
    "        self.epsilon = 0.5\n",
    "        self.activation = leaky_relu\n",
    "        self.initializer = tf.contrib.keras.initializers.he_normal()\n",
    "            \n",
    "    def build_G_net(self) :\n",
    "        with tf.variable_scope('G') as g_scope:\n",
    "            self.G_in_hair = tf.placeholder(tf.int32, shape=[None])\n",
    "            self.G_in_eyes = tf.placeholder(tf.int32, shape=[None])\n",
    "            self.G_in_noise = tf.placeholder(tf.float32, shape=[None,100])\n",
    "            \n",
    "            self.G_H_onehot = tf.one_hot(self.G_in_hair, self.hair_n)\n",
    "            self.G_E_onehot = tf.one_hot(self.G_in_eyes, self.eyes_n)\n",
    "            g = tf.concat([self.G_H_onehot, self.G_E_onehot, self.G_in_noise], axis=1)\n",
    "            g = tf.layers.dense(g,4*4*1024,activation=None)\n",
    "            g = tf.reshape(g,(-1,4,4,1024))\n",
    "            \n",
    "            g = tf.layers.conv2d_transpose(g, filters=512, kernel_size=(5,5), strides=(2,2), kernel_initializer=self.initializer,\n",
    "                                           padding='same', activation=self.activation)\n",
    "            g = tf.layers.batch_normalization(g)\n",
    "            g = tf.layers.conv2d_transpose(g, filters=256, kernel_size=(5,5), strides=(2,2), kernel_initializer=self.initializer,\n",
    "                                           padding='same', activation=self.activation)\n",
    "            g = tf.layers.batch_normalization(g)\n",
    "            g = tf.layers.conv2d_transpose(g, filters=128, kernel_size=(5,5), strides=(2,2), kernel_initializer=self.initializer,\n",
    "                                           padding='same', activation=self.activation)\n",
    "            g = tf.layers.batch_normalization(g)\n",
    "            self.g = tf.layers.conv2d_transpose(g, filters=3, kernel_size=(5,5), strides=(2,2), kernel_initializer=self.initializer,\n",
    "                                           padding='same', activation=tf.nn.tanh)\n",
    "        \n",
    "    def build_D_net(self) :\n",
    "\n",
    "        with tf.variable_scope('D') as d_scope:\n",
    "\n",
    "            ### include right and fake1\n",
    "            self.D_in_hair = tf.placeholder(tf.int32, shape=[None])\n",
    "            self.D_in_eyes = tf.placeholder(tf.int32, shape=[None])\n",
    "            self.D_in_img = tf.placeholder(tf.float32, shape=[None,64,64,3])\n",
    "#             self.D_in_img = self.D_in_img*2 - 1\n",
    "            \n",
    "            D_H_onehot = tf.one_hot(self.D_in_hair, self.hair_n)\n",
    "            D_E_onehot = tf.one_hot(self.D_in_eyes, self.eyes_n)\n",
    "            \n",
    "            self.D_in_hair_right = D_H_onehot[:self.bs]\n",
    "            self.D_in_eyes_right = D_E_onehot[:self.bs]\n",
    "            self.D_in_img_right = self.D_in_img[:self.bs]\n",
    "            \n",
    "            self.D_in_hair_fake = D_H_onehot[self.bs:]\n",
    "            self.D_in_eyes_fake = D_E_onehot[self.bs:]\n",
    "            self.D_in_img_fake = self.D_in_img[self.bs:]\n",
    "            \n",
    "            use_epsilon_uniform = 1\n",
    "            if use_epsilon_uniform :\n",
    "                self.epsilon = tf.placeholder(tf.float32, shape=[1])\n",
    "                self.D_x_hat_hair = self.epsilon*self.D_in_hair_right + (1-self.epsilon)*self.D_in_hair_fake\n",
    "                self.D_x_hat_eyes = self.epsilon*self.D_in_eyes_right + (1-self.epsilon)*self.D_in_eyes_fake\n",
    "                self.D_x_hat_img = self.epsilon*self.D_in_img_right + (1-self.epsilon)*self.D_in_img_fake\n",
    "            else :\n",
    "                self.D_x_hat_hair = self.epsilon*self.D_in_hair_right + (1-self.epsilon)*self.D_in_hair_fake\n",
    "                self.D_x_hat_eyes = self.epsilon*self.D_in_eyes_right + (1-self.epsilon)*self.D_in_eyes_fake\n",
    "                self.D_x_hat_img = self.epsilon*self.D_in_img_right + (1-self.epsilon)*self.D_in_img_fake\n",
    "                \n",
    "            self.D_img = tf.concat([self.D_in_img_right,self.D_in_img_fake,self.D_x_hat_img], axis=0)\n",
    "            self.D_hair = tf.concat([self.D_in_hair_right,self.D_in_hair_fake,self.D_x_hat_hair], axis=0)\n",
    "            self.D_eyes = tf.concat([self.D_in_eyes_right,self.D_in_eyes_fake,self.D_x_hat_eyes], axis=0)\n",
    "            \n",
    "            d = tf.layers.conv2d(self.D_img, filters=128, kernel_size=(5,5), strides=(2,2), kernel_initializer=self.initializer,\n",
    "                                 padding='same', activation=self.activation, name='d_c1', reuse=None)\n",
    "            d = tf.layers.conv2d(d, filters=256, kernel_size=(5,5), strides=(2,2), kernel_initializer=self.initializer,\n",
    "                                 padding='same', activation=self.activation, name='d_c2', reuse=None)\n",
    "            d = tf.layers.conv2d(d, filters=512, kernel_size=(5,5), strides=(2,2), kernel_initializer=self.initializer,\n",
    "                                 padding='same', activation=self.activation, name='d_c3', reuse=None)\n",
    "            d = tf.layers.conv2d(d, filters=1024, kernel_size=(5,5), strides=(2,2), kernel_initializer=self.initializer,\n",
    "                                 padding='same', activation=self.activation, name='d_c4', reuse=None)\n",
    "            \n",
    "            D_tag_in = tf.concat([self.D_hair, self.D_eyes], axis=1)\n",
    "            D_tag_in = tf.layers.dense(D_tag_in,4*4*1024,activation=self.activation, name='d_d1', kernel_initializer=self.initializer, reuse=None)\n",
    "            D_tag_in = tf.reshape(D_tag_in,(-1,4,4,1024))\n",
    "            \n",
    "            d = tf.concat([d, D_tag_in], axis=3)\n",
    "            d = tf.layers.conv2d(d, filters=256, kernel_size=(1,1), strides=(1,1), \n",
    "                                 padding='same', activation=self.activation, name='d_c5', kernel_initializer=self.initializer, reuse=None)\n",
    "            d = tf.reshape(d, [-1, 4*4*256]) \n",
    "            self.d_temp = tf.layers.dense(d, 1 ,activation=None, name='d_d2', kernel_initializer=self.initializer, reuse=None)\n",
    "        \n",
    "            self.d_right, self.d_fake, self.d_x_hat = tf.split(self.d_temp, num_or_size_splits=3, axis=0)\n",
    "#             self.d_right, self.d_fake_temp = tf.split(self.d_temp, num_or_size_splits=2, axis=0)\n",
    "#             self.d_fake, self.d_x_hat = tf.split(self.d_fake_temp, num_or_size_splits=2, axis=0)\n",
    "            \n",
    "            ### gradient penalty\n",
    "#             gradient_temp = tf.gradients(self.d_x_hat,[self.D_x_hat_img,self.D_x_hat_hair,self.D_x_hat_eyes])\n",
    "            gradient_img_temp = tf.gradients(self.d_x_hat,self.D_x_hat_img)\n",
    "            gradient_hair_temp = tf.gradients(self.d_x_hat,self.D_x_hat_hair)\n",
    "            gradient_eyes_temp = tf.gradients(self.d_x_hat,self.D_x_hat_eyes)\n",
    "#             self.gradient_penalty = tf.maximum(tf.constant(0.0, shape=[self.bs,1]), tf.sqrt(tf.square(gradient_temp)) - tf.constant(1.0, shape=[self.bs,1]))\n",
    "            self.gradient_penalty_img = tf.square(tf.sqrt(tf.reduce_sum(tf.square(gradient_img_temp), axis=1)) - 1)\n",
    "#             self.gradient_penalty_img = self.lda * tf.reduce_mean(self.gradient_penalty_img)\n",
    "            self.gradient_penalty_hair = tf.square(tf.sqrt(tf.reduce_sum(tf.square(gradient_hair_temp), axis=1)) - 1)\n",
    "#             self.gradient_penalty_hair = self.lda * tf.reduce_mean(self.gradient_penalty_hair)\n",
    "            self.gradient_penalty_eyes = tf.square(tf.sqrt(tf.reduce_sum(tf.square(gradient_eyes_temp), axis=1)) - 1)\n",
    "#             self.gradient_penalty_eyes = self.lda * tf.reduce_mean(self.gradient_penalty_eyes)\n",
    "            self.gradient_penalty = self.lda * (tf.reduce_mean(self.gradient_penalty_img)\n",
    "                                                + tf.reduce_mean(self.gradient_penalty_hair)/10\n",
    "                                                + tf.reduce_mean(self.gradient_penalty_eyes)/10)\n",
    "            \n",
    "            ### final loss\n",
    "            self.d_loss = -(tf.reduce_mean(self.d_right) - tf.reduce_mean(self.d_fake) - self.gradient_penalty)\n",
    "            self.d_train_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,\"D/\")\n",
    "            self.d_train = tf.train.AdamOptimizer(learning_rate=self.lr,beta1=0.0,beta2=0.9).minimize(self.d_loss, var_list=self.d_train_vars)\n",
    "            \n",
    "            ### G_D loss\n",
    "            self.G_D_in_hair = self.G_in_hair\n",
    "            self.G_D_in_eyes = self.G_in_eyes\n",
    "            self.G_D_in_img = self.g\n",
    "            \n",
    "            self.G_D_eyes = tf.one_hot(self.G_D_in_eyes, self.eyes_n)\n",
    "            self.G_D_hair = tf.one_hot(self.G_D_in_hair, self.hair_n)\n",
    "            \n",
    "            g_d = tf.layers.conv2d(self.G_D_in_img, filters=128, kernel_size=(5,5), strides=(2,2), kernel_initializer=self.initializer,\n",
    "                                 padding='same', activation=self.activation, name='d_c1', reuse=True)\n",
    "            g_d = tf.layers.conv2d(g_d, filters=256, kernel_size=(5,5), strides=(2,2), kernel_initializer=self.initializer,\n",
    "                                 padding='same', activation=self.activation, name='d_c2', reuse=True)\n",
    "            g_d = tf.layers.conv2d(g_d, filters=512, kernel_size=(5,5), strides=(2,2), kernel_initializer=self.initializer,\n",
    "                                 padding='same', activation=self.activation, name='d_c3', reuse=True)\n",
    "            g_d = tf.layers.conv2d(g_d, filters=1024, kernel_size=(5,5), strides=(2,2), kernel_initializer=self.initializer,\n",
    "                                 padding='same', activation=self.activation, name='d_c4', reuse=True)\n",
    "            \n",
    "            G_D_tag_in = tf.concat([self.G_D_hair, self.G_D_eyes], axis=1)\n",
    "            G_D_tag_in = tf.layers.dense(G_D_tag_in,4*4*1024,activation=self.activation, name='d_d1', kernel_initializer=self.initializer, reuse=True)\n",
    "            G_D_tag_in = tf.reshape(G_D_tag_in,(-1,4,4,1024))\n",
    "            \n",
    "            g_d = tf.concat([g_d, G_D_tag_in], axis=3)\n",
    "            g_d = tf.layers.conv2d(g_d, filters=256, kernel_size=(1,1), strides=(1,1), \n",
    "                                 padding='same', activation=self.activation, name='d_c5', kernel_initializer=self.initializer, reuse=True)\n",
    "            g_d = tf.reshape(g_d, [-1, 4*4*256]) \n",
    "            self.g_d_temp = tf.layers.dense(g_d, 1 ,activation=None, name='d_d2', kernel_initializer=self.initializer, reuse=True)\n",
    "            \n",
    "            self.gd_loss = - tf.reduce_mean(self.g_d_temp)\n",
    "            \n",
    "            #\n",
    "            # HE classification\n",
    "            #\n",
    "            \n",
    "            # hair classification pretrained\n",
    "            self.hair_in_hair = tf.placeholder(tf.int32, shape=[None])\n",
    "            self.hair_in_img = tf.placeholder(tf.float32, shape=[None,64,64,3]) \n",
    "\n",
    "            hair_H_onehot = tf.one_hot(self.hair_in_hair, self.hair_n)\n",
    "            hair_temp = tf.layers.batch_normalization(self.hair_in_img, name='hair_b0', reuse=None)\n",
    "            hair_temp = tf.layers.conv2d(hair_temp, filters=32, kernel_size=(5,5), strides=(2,2), kernel_initializer=self.initializer,\n",
    "                                 padding='same', activation=self.activation, name='hair_c1', reuse=None)\n",
    "            hair_temp = tf.layers.batch_normalization(hair_temp, name='hair_b1', reuse=None)\n",
    "            hair_temp = tf.layers.conv2d(hair_temp, filters=32, kernel_size=(5,5), strides=(2,2), kernel_initializer=self.initializer,\n",
    "                                 padding='same', activation=self.activation, name='hair_c2', reuse=None)\n",
    "            hair_temp = tf.layers.batch_normalization(hair_temp, name='hair_b2', reuse=None)\n",
    "            hair_temp = tf.layers.conv2d(hair_temp, filters=32, kernel_size=(5,5), strides=(2,2), kernel_initializer=self.initializer,\n",
    "                                 padding='same', activation=self.activation, name='hair_c3', reuse=None)\n",
    "            hair_temp = tf.layers.batch_normalization(hair_temp, name='hair_b3', reuse=None)\n",
    "            hair_temp = tf.layers.conv2d(hair_temp, filters=32, kernel_size=(5,5), strides=(2,2), kernel_initializer=self.initializer,\n",
    "                                 padding='same', activation=self.activation, name='hair_c4', reuse=None)\n",
    "            hair_temp = tf.reshape(hair_temp, [-1, 4*4*32]) \n",
    "            self.hair = tf.layers.dense(hair_temp, self.hair_n ,activation=tf.nn.softmax, name='hair_s2', kernel_initializer=self.initializer, reuse=None)\n",
    "\n",
    "            self.hair_loss = tf.reduce_mean(-tf.reduce_sum(hair_H_onehot * tf.log(self.hair), axis=1))\n",
    "            self.hair_train_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,\"D/\")\n",
    "            self.hair_train = tf.train.AdamOptimizer(learning_rate=self.lr_pre).minimize(self.hair_loss, var_list=self.hair_train_vars)\n",
    "            \n",
    "            # eyes classification pretrained\n",
    "            self.eyes_in_eyes = tf.placeholder(tf.int32, shape=[None])\n",
    "            self.eyes_in_img = tf.placeholder(tf.float32, shape=[None,64,64,3]) \n",
    "\n",
    "            eyes_E_onehot = tf.one_hot(self.eyes_in_eyes, self.eyes_n)\n",
    "            eyes_temp = tf.layers.batch_normalization(self.eyes_in_img, name='eyes_b0', reuse=None)\n",
    "            eyes_temp = tf.layers.conv2d(eyes_temp, filters=32, kernel_size=(5,5), strides=(2,2), kernel_initializer=self.initializer,\n",
    "                                 padding='same', activation=self.activation, name='eyes_c1', reuse=None)\n",
    "            eyes_temp = tf.layers.batch_normalization(eyes_temp, name='eyes_b1', reuse=None)\n",
    "            eyes_temp = tf.layers.conv2d(eyes_temp, filters=32, kernel_size=(5,5), strides=(2,2), kernel_initializer=self.initializer,\n",
    "                                 padding='same', activation=self.activation, name='eyes_c2', reuse=None)\n",
    "            eyes_temp = tf.layers.batch_normalization(eyes_temp, name='eyes_b2', reuse=None)\n",
    "            eyes_temp = tf.layers.conv2d(eyes_temp, filters=32, kernel_size=(5,5), strides=(2,2), kernel_initializer=self.initializer,\n",
    "                                 padding='same', activation=self.activation, name='eyes_c3', reuse=None)\n",
    "            eyes_temp = tf.layers.batch_normalization(eyes_temp, name='eyes_b3', reuse=None)\n",
    "            eyes_temp = tf.layers.conv2d(eyes_temp, filters=32, kernel_size=(5,5), strides=(2,2), kernel_initializer=self.initializer,\n",
    "                                 padding='same', activation=self.activation, name='eyes_c4', reuse=None)\n",
    "            eyes_temp = tf.reshape(eyes_temp, [-1, 4*4*32]) \n",
    "            self.eyes = tf.layers.dense(eyes_temp, self.eyes_n ,activation=tf.nn.softmax, name='eyes_s2', kernel_initializer=self.initializer, reuse=None)\n",
    "\n",
    "            self.eyes_loss = tf.reduce_mean(-tf.reduce_sum(eyes_E_onehot * tf.log(self.eyes), axis=1))\n",
    "            self.eyes_train_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,\"D/\")\n",
    "            self.eyes_train = tf.train.AdamOptimizer(learning_rate=self.lr_pre).minimize(self.eyes_loss, var_list=self.eyes_train_vars)\n",
    "            \n",
    "            ### hair classification\n",
    "            gh_temp = tf.layers.batch_normalization(self.g, name='hair_b0', reuse=True)\n",
    "            gh_temp = tf.layers.conv2d(gh_temp, filters=32, kernel_size=(5,5), strides=(2,2), kernel_initializer=self.initializer,\n",
    "                                 padding='same', activation=self.activation, name='hair_c1', reuse=True)\n",
    "            gh_temp = tf.layers.batch_normalization(gh_temp, name='hair_b1', reuse=True)\n",
    "            gh_temp = tf.layers.conv2d(gh_temp, filters=32, kernel_size=(5,5), strides=(2,2), kernel_initializer=self.initializer,\n",
    "                                 padding='same', activation=self.activation, name='hair_c2', reuse=True)\n",
    "            gh_temp = tf.layers.batch_normalization(gh_temp, name='hair_b2', reuse=True)\n",
    "            gh_temp = tf.layers.conv2d(gh_temp, filters=32, kernel_size=(5,5), strides=(2,2), kernel_initializer=self.initializer,\n",
    "                                 padding='same', activation=self.activation, name='hair_c3', reuse=True)\n",
    "            gh_temp = tf.layers.batch_normalization(gh_temp, name='hair_b3', reuse=True)\n",
    "            gh_temp = tf.layers.conv2d(gh_temp, filters=32, kernel_size=(5,5), strides=(2,2), kernel_initializer=self.initializer,\n",
    "                                 padding='same', activation=self.activation, name='hair_c4', reuse=True)\n",
    "            gh_temp = tf.reshape(gh_temp, [-1, 4*4*32]) \n",
    "            self.gh = tf.layers.dense(gh_temp, self.hair_n ,activation=tf.nn.softmax, name='hair_s2', kernel_initializer=self.initializer, reuse=True)\n",
    "            \n",
    "            self.gh_loss = tf.reduce_mean(-tf.reduce_sum(self.G_H_onehot * tf.log(self.gh), axis=1))\n",
    "            \n",
    "            ### eyes classification\n",
    "            ge_temp = tf.layers.batch_normalization(self.g, name='eyes_b0', reuse=True)\n",
    "            ge_temp = tf.layers.conv2d(ge_temp, filters=32, kernel_size=(5,5), strides=(2,2), kernel_initializer=self.initializer,\n",
    "                                 padding='same', activation=self.activation, name='eyes_c1', reuse=True)\n",
    "            ge_temp = tf.layers.batch_normalization(ge_temp, name='eyes_b1', reuse=True)\n",
    "            ge_temp = tf.layers.conv2d(ge_temp, filters=32, kernel_size=(5,5), strides=(2,2), kernel_initializer=self.initializer,\n",
    "                                 padding='same', activation=self.activation, name='eyes_c2', reuse=True)\n",
    "            ge_temp = tf.layers.batch_normalization(ge_temp, name='eyes_b2', reuse=True)\n",
    "            ge_temp = tf.layers.conv2d(ge_temp, filters=32, kernel_size=(5,5), strides=(2,2), kernel_initializer=self.initializer,\n",
    "                                 padding='same', activation=self.activation, name='eyes_c3', reuse=True)\n",
    "            ge_temp = tf.layers.batch_normalization(ge_temp, name='eyes_b3', reuse=True)\n",
    "            ge_temp = tf.layers.conv2d(ge_temp, filters=32, kernel_size=(5,5), strides=(2,2), kernel_initializer=self.initializer,\n",
    "                                 padding='same', activation=self.activation, name='eyes_c4', reuse=True)\n",
    "            ge_temp = tf.reshape(ge_temp, [-1, 4*4*32]) \n",
    "            self.ge = tf.layers.dense(ge_temp, self.eyes_n ,activation=tf.nn.softmax, name='eyes_s2', kernel_initializer=self.initializer, reuse=True)\n",
    "\n",
    "            self.ge_loss = tf.reduce_mean(-tf.reduce_sum(self.G_E_onehot * tf.log(self.ge), axis=1))\n",
    "            \n",
    "            \n",
    "            \n",
    "            self.gd_loss_final = 1*self.gd_loss + 1*self.gh_loss + 1*self.ge_loss\n",
    "            \n",
    "            self.gd_train_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,\"G/\")\n",
    "            self.gd_train = tf.train.AdamOptimizer(learning_rate=self.lr,beta1=0.0,beta2=0.9).minimize(self.gd_loss_final, var_list=self.gd_train_vars)\n",
    "            \n",
    "    def build_net(self) :\n",
    "        self.build_G_net()\n",
    "        self.build_D_net()\n",
    "        \n",
    "        self.sess = tf.Session(config=config)\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        self.saver = tf.train.Saver()\n",
    "        tf.summary.FileWriter(\"logs/\", self.sess.graph)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_pre(text) :\n",
    "    text = text.replace(',',' ')\n",
    "    lst_text = text.split(' ')\n",
    "    hair_i = 0\n",
    "    eyes_i = 0\n",
    "    for i,s in enumerate(lst_text) :\n",
    "        if s == 'hair' :\n",
    "            hair_i = i\n",
    "        elif s == 'eyes' :\n",
    "            eyes_i = i\n",
    "    if hair_i :\n",
    "        hair_style = lst_text[hair_i-1] + ' ' + lst_text[hair_i]\n",
    "    else :\n",
    "        hair_style = 'null'\n",
    "    if eyes_i :\n",
    "        eyes_style = lst_text[eyes_i-1] + ' ' + lst_text[eyes_i]\n",
    "    else :\n",
    "        eyes_style = 'null'\n",
    "    \n",
    "    return hair_style, eyes_style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_img(testing_text_id,img_n=40,hair_style='null',eyes_style='null', gan=None) :\n",
    "    seed_i = 3\n",
    "    random.seed(seed_i)\n",
    "    np.random.seed(seed_i)\n",
    "    tf.set_random_seed(seed_i)\n",
    "    lst_hair = ['null', 'orange hair', 'white hair', 'aqua hair', 'gray hair',\n",
    "                'green hair', 'red hair', 'purple hair', 'pink hair',\n",
    "                'blue hair', 'black hair', 'brown hair', 'blonde hair']\n",
    "    lst_eyes = ['null', 'gray eyes', 'black eyes', 'orange eyes',\n",
    "                'pink eyes', 'yellow eyes', 'aqua eyes', 'purple eyes',\n",
    "                'green eyes', 'brown eyes', 'red eyes', 'blue eyes']\n",
    "    \n",
    "    hair_i = 0 \n",
    "    eyes_i = 0\n",
    "    for i,h in enumerate(lst_hair) :\n",
    "        if h == hair_style :\n",
    "            hair_i = i\n",
    "    for i,e in enumerate(lst_eyes) :\n",
    "        if e == eyes_style :\n",
    "            eyes_i = i\n",
    "            \n",
    "#     if hair_i == 6 :\n",
    "#         seed_i = \n",
    "    \n",
    "    \n",
    "    if hair_i != 0 :\n",
    "        gen_tag_hair = np.zeros((int(img_n),), dtype=np.int) + hair_i\n",
    "    else :\n",
    "        gen_tag_hair = np.random.randint(1,13,size=img_n)\n",
    "    if eyes_i != 0 :\n",
    "        gen_tag_eyes = np.zeros((int(img_n),), dtype=np.int) + eyes_i\n",
    "    else :\n",
    "        gen_tag_eyes = np.random.randint(1,12,size=img_n)\n",
    "    \n",
    "    ary_temp = np.random.normal(0,1,[img_n,100])\n",
    "    b_img = gan.sess.run(gan.g, feed_dict={gan.G_in_hair:gen_tag_hair, \n",
    "                                           gan.G_in_eyes:gen_tag_eyes, \n",
    "                                           gan.G_in_noise:ary_temp})\n",
    "    b_img = (b_img + 1.0) / 2.0\n",
    "        \n",
    "    if not os.path.isdir('./samples') :\n",
    "        os.makedirs('./samples')\n",
    "    \n",
    "    if testing_text_id == '1' :\n",
    "        lst_img = b_img[[3,8,10,11,16]]\n",
    "        for i,img in enumerate(lst_img) :\n",
    "            scipy.misc.imsave('./samples/sample_{}_{}.jpg'.format(int(testing_text_id),i+1),img)\n",
    "    if testing_text_id == '2' :\n",
    "        lst_img = b_img[[7,13,14,29,32]]\n",
    "        for i,img in enumerate(lst_img) :\n",
    "            scipy.misc.imsave('./samples/sample_{}_{}.jpg'.format(int(testing_text_id),i+1),img)\n",
    "\n",
    "    if testing_text_id == '3' :\n",
    "        lst_img = b_img[[1,4,16,18,25]]\n",
    "        for i,img in enumerate(lst_img) :\n",
    "            scipy.misc.imsave('./samples/sample_{}_{}.jpg'.format(int(testing_text_id),i+1),img)\n",
    "\n",
    "#     for i,img in enumerate(b_img) :\n",
    "#         scipy.misc.imsave('./samples/sample_{}_{}.jpg'.format(int(testing_text_id),i+1),img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/testing_text.txt\n",
      "INFO:tensorflow:Restoring parameters from ./model_tf/model_gan9_bs16_HEclass_original.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nfs/nas-5.1/thchu/anaconda3/envs/py35/lib/python3.5/site-packages/ipykernel_launcher.py:47: DeprecationWarning: `imsave` is deprecated!\n",
      "`imsave` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imwrite`` instead.\n",
      "/nfs/nas-5.1/thchu/anaconda3/envs/py35/lib/python3.5/site-packages/ipykernel_launcher.py:51: DeprecationWarning: `imsave` is deprecated!\n",
      "`imsave` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imwrite`` instead.\n",
      "/nfs/nas-5.1/thchu/anaconda3/envs/py35/lib/python3.5/site-packages/ipykernel_launcher.py:56: DeprecationWarning: `imsave` is deprecated!\n",
      "`imsave` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imwrite`` instead.\n"
     ]
    }
   ],
   "source": [
    "print (text_path)\n",
    "tf.reset_default_graph()\n",
    "gan = GAN()\n",
    "gan.build_net()\n",
    "gan.saver.restore(gan.sess, './model_tf/model_gan9_bs16_HEclass_original.ckpt')\n",
    "# gan.saver.restore(gan.sess, './model_tf/model_gan9_bs16_HEclass_more.ckpt')\n",
    "# gan.saver.restore(gan.sess, './model_tf/model_gan.ckpt')\n",
    "with open(text_path, 'r') as f :\n",
    "#     lst_text = f.readlines()\n",
    "    for text in f.readlines() :\n",
    "        lst_temp = text.split(',')\n",
    "        testing_text_id = lst_temp[0]\n",
    "        testing_text = lst_temp[1]\n",
    "        hair_style, eyes_style = text_pre(text)\n",
    "        generate_img(testing_text_id=testing_text_id,hair_style=hair_style,eyes_style=eyes_style,gan=gan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
