{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import torch\n",
    "import pickle\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.layers import *\n",
    "from keras.models import *\n",
    "from keras import backend as K\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt \n",
    "# def init():\n",
    "#     config = tf.ConfigProto()\n",
    "#     config.gpu_options.allow_growth = True\n",
    "#     session = tf.Session(config=config)\n",
    "#     keras.backend.tensorflow_backend.set_session(session)\n",
    "# init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "### path and parameter\n",
    "path_data = './MLDS_hw2_data/'\n",
    "str_output = 'special.txt'\n",
    "model_name = 's2s'\n",
    "max_seq = 8\n",
    "n_caption = -1\n",
    "\n",
    "loading_model = 1\n",
    "do_training = 0\n",
    "teacherForce = 0\n",
    "\n",
    "save_model = 0\n",
    "train_data_loading = 0\n",
    "test_data_loading = 1\n",
    "peer_review_data_loading = 0\n",
    "\n",
    "special_task = 1\n",
    "\n",
    "if len(sys.argv) > 1 :\n",
    "    path_data = sys.argv[1]\n",
    "    str_output = sys.argv[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lang:\n",
    "    def __init__(self):\n",
    "        self.word2index = {\"<BOS>\": 0, \"<EOS>\" :1}\n",
    "        self.word2count = {\"<BOS>\": 0, \"<EOS>\" : 0}\n",
    "        self.index2word = {0: \"<BOS>\", 1: \"<EOS>\"}\n",
    "        self.n_words = 2  # Count BOS and EOS\n",
    "        self.max_len_seq = 0\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        lst_word = sentence.split()\n",
    "        if len(lst_word) > max_seq :\n",
    "            return 0\n",
    "        elif self.max_len_seq < len(lst_word) :\n",
    "            self.max_len_seq = len(lst_word)\n",
    "        for word in lst_word :\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keras_log_plot(log) :\n",
    "    matplotlib.rcParams.update({'font.size': 16})\n",
    "    fig = plt.figure(1,figsize=(20,10))\n",
    "    \n",
    "    plt.subplot(121)\n",
    "    plt.plot(log['acc'], label='train_acc')\n",
    "    plt.plot(log['val_acc'], label='val_acc')\n",
    "    plt.legend(fontsize=20)\n",
    "    plt.xlabel('epoch', fontsize=20, color='black')\n",
    "    plt.ylabel('acc', fontsize=20, color='black')\n",
    "\n",
    "    plt.subplot(122)\n",
    "    plt.plot(log['loss'], label='train_loss')\n",
    "    plt.plot(log['val_loss'], label='val_loss')\n",
    "    plt.legend(fontsize=20)\n",
    "    plt.xlabel('epoch', fontsize=20, color='black')\n",
    "    plt.ylabel('loss', fontsize=20, color='black')\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lang_init(lst_dict_label_train) :\n",
    "    lang = Lang()\n",
    "    for dict_label_train in lst_dict_label_train :\n",
    "        for sentence in dict_label_train['caption'] :\n",
    "            sentence = sentence[:-1] + ' <EOS>'\n",
    "            lang.addSentence(sentence) # remove \".\"\n",
    "    print (lang.n_words)\n",
    "    print (lang.max_len_seq)\n",
    "    assert lang.max_len_seq == max_seq, 'error here'\n",
    "    return lang\n",
    "# assert lang.word2count['<BOS>'] == lang.word2count['<EOS>'], number of \"<BOS>\" != number of \"<EOS>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### index to one-hot\n",
    "def Str2OneHot(sentence, n_class, dict_map, max_len_seq) :\n",
    "    ### sentence to lst_index_sentence\n",
    "    sentence = sentence[:-1] + ' <EOS>'\n",
    "    lst_word = sentence.split()\n",
    "    ary_oneHot = np.zeros((max_len_seq,n_class))\n",
    "    lst_index_word = [dict_map[word] for word in lst_word]\n",
    "    ary_oneHot[range(len(lst_word)),lst_index_word] = 1\n",
    "    ary_oneHot[range(len(lst_word),len(ary_oneHot)),:] = 0.0 # others all set <EOS>\n",
    "    return ary_oneHot\n",
    "    \n",
    "### just for test\n",
    "# ary = Str2OneHot('A woman goes under a horse.', lang.n_words, lang.word2index, lang.max_len_seq)\n",
    "# print (ary.shape)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "training_label.json : \n",
      "caption : \n",
      "['A woman goes under a horse.', 'A woman crawls under a horse and gets a surprise.']\n",
      "id : \n",
      "xBePrplM4OA_6_18.avi\n",
      "4011\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "### preprocessing\n",
    "\n",
    "### for training data\n",
    "### load lst_dict_label_train\n",
    "\n",
    "with open('{}training_label.json'.format(path_data)) as f :\n",
    "    lst_dict_label_train = json.load(f)\n",
    "print ('\\ntraining_label.json : ')\n",
    "print ('caption : \\n' + str(lst_dict_label_train[0]['caption'][:2]))\n",
    "print ('id : \\n' + str(lst_dict_label_train[0]['id']))\n",
    "\n",
    "lang = lang_init(lst_dict_label_train)\n",
    "\n",
    "if train_data_loading :\n",
    "    ### pair caption and vedio feature\n",
    "    ### output : ary_train_EC_input\n",
    "    ###          ary_train_DC_output\n",
    "    ###          ary_train_DC_input\n",
    "    lst_id_train = [dict_label_train['id'] for dict_label_train in lst_dict_label_train]\n",
    "    lst_train_EC_input = []\n",
    "    lst_train_DC_output = []\n",
    "    for i, id in enumerate(lst_id_train) :\n",
    "        lst_npy = np.load('{}training_data/feat/{}.npy'.format(path_data, id)).tolist()\n",
    "        for caption in lst_dict_label_train[i]['caption'][:n_caption] :\n",
    "            if len(caption.split()) >= max_seq : # note >=\n",
    "                continue\n",
    "            lst_train_EC_input += [lst_npy]\n",
    "            lst_ary_OneHot = Str2OneHot(caption, lang.n_words, lang.word2index, lang.max_len_seq).tolist()\n",
    "            lst_train_DC_output += [lst_ary_OneHot]\n",
    "    assert len(lst_train_EC_input) == len(lst_train_DC_output), \"??\"\n",
    "    # ary_train_EC_input = np.concatenate(lst_train_EC_input,axis=0).reshape(-1,80,4096)\n",
    "    # ary_train_DC_output = np.concatenate(lst_train_DC_output,axis=0).reshape(-1,lang.max_len_seq,lang.n_words)\n",
    "    print ('here')\n",
    "    ary_train_EC_input = np.asarray(lst_train_EC_input).reshape(-1,80,4096)\n",
    "    print ('go go go')\n",
    "    del lst_train_EC_input\n",
    "    ary_train_DC_output = np.asarray(lst_train_DC_output).reshape(-1,lang.max_len_seq,lang.n_words)\n",
    "    del lst_train_DC_output\n",
    "\n",
    "    print (ary_train_EC_input.shape)\n",
    "    print (ary_train_DC_output.shape)\n",
    "\n",
    "    ### add \"<BOS>\" to ary_train_DC_input\n",
    "    ary_temp = np.zeros((len(ary_train_EC_input),1,lang.n_words))\n",
    "    ary_temp[:,0,0] = 1\n",
    "    if teacherForce :\n",
    "        ary_train_DC_input = np.concatenate([ary_temp,ary_train_DC_output[:,:-1]],axis=1)\n",
    "    else :\n",
    "        ary_train_DC_input = ary_temp\n",
    "\n",
    "\n",
    "### for testing data\n",
    "if test_data_loading :\n",
    "    with open('{}testing_label.json'.format(path_data)) as f :\n",
    "        lst_dict_label_test = json.load(f)\n",
    "    ### pair caption and vedio feature\n",
    "    ### output : ary_test_EC_input\n",
    "    lst_id_test = [dict_label_test['id'] for dict_label_test in lst_dict_label_test]\n",
    "\n",
    "    ### just check\n",
    "    lst_id_test_2 = []\n",
    "    with open('{}{}.txt'.format(path_data, 'testing_id')) as f :\n",
    "        for line in f.readlines() :\n",
    "            lst_id_test_2 += [line.rstrip('\\n')]\n",
    "    for i in range(len(lst_id_test)) :\n",
    "        assert str(lst_id_test[i]) == str(lst_id_test_2[i]), 'error here'\n",
    "\n",
    "    lst_test_EC_input = []\n",
    "    for i, id in enumerate(lst_id_test) :\n",
    "        npy = np.load('{}testing_data/feat/{}.npy'.format(path_data, id))\n",
    "        lst_test_EC_input += [npy]\n",
    "    ary_test_EC_input = np.concatenate(lst_test_EC_input,axis=0).reshape(-1,80,4096)\n",
    "\n",
    "    ary_temp = np.zeros((len(ary_test_EC_input),1,lang.n_words))\n",
    "    ary_temp[:,0,0] = 1\n",
    "    ary_test_DC_input = ary_temp\n",
    "    \n",
    "### for peer review\n",
    "if peer_review_data_loading :\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_pretrain(lang=lang) :\n",
    "\n",
    "    EC_input = Input(shape=(80,4096))\n",
    "    EC_output = GRU(32,return_state=False, return_sequences=True, activation='selu')(EC_input)\n",
    "    EC_output, EC_output_state = GRU(32,return_state=True, activation='selu')(EC_output)\n",
    "    DC_input = Input(shape=(None,lang.n_words))\n",
    "    DC_input_M = Masking(mask_value=0.0)(DC_input)\n",
    "\n",
    "    DC_gru1 = GRU(32, return_sequences=True, activation='selu')\n",
    "    DC_time_dense = TimeDistributed(Dense(lang.n_words, activation='softmax'))\n",
    "\n",
    "    DC_output = DC_gru1(DC_input_M, initial_state=EC_output_state)\n",
    "    DC_output = DC_time_dense(DC_output)\n",
    "\n",
    "    model = Model([EC_input,DC_input],DC_output)\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
    "    print (model.summary())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_DC_only1(lang=lang) :\n",
    "    EC_input = Input(shape=(80,4096))\n",
    "    EC_output = GRU(32,return_state=False, return_sequences=True, activation='selu')(EC_input)\n",
    "    EC_output, EC_output_state = GRU(32,return_state=True, activation='selu')(EC_output)\n",
    "    DC_input = Input(shape=(1,lang.n_words))\n",
    "    DC_input_M = Masking(mask_value=0.0)(DC_input)\n",
    "\n",
    "    DC_gru1 = GRU(32, return_sequences=True, return_state=True, activation='selu')\n",
    "    DC_time_dense = TimeDistributed(Dense(lang.n_words, activation='softmax'))\n",
    "\n",
    "    DC_output_state = EC_output_state\n",
    "    DC_output = DC_input_M\n",
    "    lst_DC_output = []\n",
    "    for _ in range(lang.max_len_seq) :\n",
    "        DC_output, DC_output_state = DC_gru1(DC_output, initial_state=DC_output_state)\n",
    "        DC_output = DC_time_dense(DC_output)\n",
    "        lst_DC_output += [DC_output]\n",
    "\n",
    "    DC_output = Lambda(lambda x: K.concatenate(x, axis=1))(lst_DC_output)\n",
    "\n",
    "    model = Model([EC_input,DC_input],DC_output)\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
    "    print (model.summary())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def model_DC_only1(lang=lang) :\n",
    "#     EC_input = Input(shape=(80,4096))\n",
    "#     EC_output = GRU(64,return_state=False, return_sequences=True, activation='selu')(EC_input)\n",
    "#     EC_output = BatchNormalization()(EC_output)\n",
    "#     EC_output = GRU(64,return_state=False, return_sequences=True, activation='selu')(EC_output)\n",
    "#     EC_output = BatchNormalization()(EC_output)\n",
    "#     EC_output, EC_output_state = GRU(64,return_state=True, activation='selu')(EC_output)\n",
    "#     EC_output_stage = BatchNormalization()(EC_output_state)\n",
    "#     DC_input = Input(shape=(1,lang.n_words))\n",
    "# #     DC_input_M = Masking(mask_value=0.0)(DC_input)\n",
    "\n",
    "#     DC_gru1 = GRU(64, return_sequences=True, return_state=True, activation='selu')\n",
    "#     DC_gru2 = GRU(64, return_sequences=True, return_state=True, activation='selu')\n",
    "#     DC_gru3 = GRU(64, return_sequences=True, return_state=True, activation='selu')\n",
    "#     DC_time_dense = TimeDistributed(Dense(lang.n_words, activation='softmax'))\n",
    "#     Ba_output_1 = TimeDistributed(BatchNormalization())\n",
    "#     Ba_output_2 = TimeDistributed(BatchNormalization())\n",
    "#     Ba_output_3 = TimeDistributed(BatchNormalization())\n",
    "\n",
    "#     DC_output_state1 = EC_output_state\n",
    "#     DC_output_state2 = EC_output_state\n",
    "#     DC_output_state3 = EC_output_state\n",
    "#     DC_output = DC_input\n",
    "#     lst_DC_output = []\n",
    "#     for _ in range(lang.max_len_seq) :\n",
    "#         DC_output, DC_output_state1 = DC_gru1(DC_output, initial_state=DC_output_state1)\n",
    "#         DC_output = Ba_output_1(DC_output)\n",
    "#         DC_output, DC_output_state2 = DC_gru2(DC_output, initial_state=DC_output_state2)\n",
    "#         DC_output = Ba_output_2(DC_output)\n",
    "#         DC_output, DC_output_state3 = DC_gru3(DC_output, initial_state=DC_output_state3)\n",
    "#         DC_output = Ba_output_3(DC_output)\n",
    "#         DC_output = DC_time_dense(DC_output)\n",
    "#         lst_DC_output += [DC_output]\n",
    "\n",
    "#     DC_output = Lambda(lambda x: K.concatenate(x, axis=1))(lst_DC_output)\n",
    "\n",
    "#     model = Model([EC_input,DC_input],DC_output)\n",
    "#     model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
    "#     print (model.summary())\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ary_pred_to_sentence(ary_pred) :\n",
    "    ary_pred_argmax = np.argmax(ary_pred,axis=2)\n",
    "    lst_ans_numbers = ary_pred_argmax.tolist()\n",
    "    lst_ans_string = []\n",
    "    for ans_numbers in lst_ans_numbers :\n",
    "        lst_ans_string += [' '.join([lang.index2word[ans] for ans in ans_numbers])]\n",
    "    return lst_ans_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_1 (InputLayer)             (None, 80, 4096)      0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_2 (InputLayer)             (None, 1, 4011)       0                                            \n",
      "____________________________________________________________________________________________________\n",
      "gru_1 (GRU)                      (None, 80, 32)        396384      input_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "masking_1 (Masking)              (None, 1, 4011)       0           input_2[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "gru_2 (GRU)                      [(None, 32), (None, 3 6240        gru_1[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "gru_3 (GRU)                      [(None, 1, 32), (None 388224      masking_1[0][0]                  \n",
      "                                                                   gru_2[0][1]                      \n",
      "                                                                   time_distributed_1[0][0]         \n",
      "                                                                   gru_3[0][1]                      \n",
      "                                                                   time_distributed_1[1][0]         \n",
      "                                                                   gru_3[1][1]                      \n",
      "                                                                   time_distributed_1[2][0]         \n",
      "                                                                   gru_3[2][1]                      \n",
      "                                                                   time_distributed_1[3][0]         \n",
      "                                                                   gru_3[3][1]                      \n",
      "                                                                   time_distributed_1[4][0]         \n",
      "                                                                   gru_3[4][1]                      \n",
      "                                                                   time_distributed_1[5][0]         \n",
      "                                                                   gru_3[5][1]                      \n",
      "                                                                   time_distributed_1[6][0]         \n",
      "                                                                   gru_3[6][1]                      \n",
      "____________________________________________________________________________________________________\n",
      "time_distributed_1 (TimeDistribu (None, 1, 4011)       132363      gru_3[0][0]                      \n",
      "                                                                   gru_3[1][0]                      \n",
      "                                                                   gru_3[2][0]                      \n",
      "                                                                   gru_3[3][0]                      \n",
      "                                                                   gru_3[4][0]                      \n",
      "                                                                   gru_3[5][0]                      \n",
      "                                                                   gru_3[6][0]                      \n",
      "                                                                   gru_3[7][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)                (None, 8, 4011)       0           time_distributed_1[0][0]         \n",
      "                                                                   time_distributed_1[1][0]         \n",
      "                                                                   time_distributed_1[2][0]         \n",
      "                                                                   time_distributed_1[3][0]         \n",
      "                                                                   time_distributed_1[4][0]         \n",
      "                                                                   time_distributed_1[5][0]         \n",
      "                                                                   time_distributed_1[6][0]         \n",
      "                                                                   time_distributed_1[7][0]         \n",
      "====================================================================================================\n",
      "Total params: 923,211\n",
      "Trainable params: 923,211\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n",
      "None\n",
      "loading and seting weight...\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "### main\n",
    "# MCP = keras.callbacks.ModelCheckpoint('./model/{}_{}.h5'.format(model_name,k), monitor='val_acc', verbose=0, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n",
    "# ES = keras.callbacks.EarlyStopping(monitor='val_acc', min_delta=0, patience=50, verbose=0, mode='auto')\n",
    "\n",
    "if teacherForce :\n",
    "    model = model_pretrain(lang)\n",
    "else :\n",
    "    model = model_DC_only1(lang)\n",
    "    if do_training :\n",
    "        ary_temp = np.zeros((len(ary_train_EC_input),1,lang.n_words))\n",
    "        ary_temp[:,0,0] = 1\n",
    "        ary_train_DC_input = ary_temp\n",
    "    \n",
    "    \n",
    "if loading_model :\n",
    "    print ('loading and seting weight...')\n",
    "    if special_task :\n",
    "        with open('./model_weight/lst_layer_weights_special.pkl'.format(i), \"rb\") as f:\n",
    "            lst_layer_weights = pickle.load(f)\n",
    "    elif teacherForce :\n",
    "        with open('./weights/lst_layer_weights.pkl'.format(i), \"rb\") as f:\n",
    "            lst_layer_weights = pickle.load(f)\n",
    "    else :\n",
    "#         with open('./weights/lst_layer_weights_noTeacher_special.pkl'.format(i), \"rb\") as f:\n",
    "        with open('./weights/lst_layer_weights.pkl'.format(i), \"rb\") as f:\n",
    "            lst_layer_weights = pickle.load(f)\n",
    "    for i, layer in enumerate(model.layers) :\n",
    "        if i > 6 :\n",
    "            break\n",
    "        print (i)\n",
    "        layer.set_weights(lst_layer_weights[i])\n",
    "\n",
    "if do_training :\n",
    "    log = model.fit([ary_train_EC_input, ary_train_DC_input],ary_train_DC_output, epochs=30, batch_size=128, validation_split=0., callbacks=[]) \n",
    "    df_log = log.history\n",
    "    fig = keras_log_plot(df_log)\n",
    "    \n",
    "if save_model :\n",
    "    print ('saving model...')\n",
    "    if not os.path.isdir('./model_weight') :\n",
    "        os.mkdir('./model_weight')\n",
    "    k = 0\n",
    "    while 1 :\n",
    "        if os.path.isfile('./model_weight/lst_layer_weights_{}.h5'.format(k)) :\n",
    "            k += 1\n",
    "        else :\n",
    "            break\n",
    "    lst_weights = []\n",
    "    for i, layer in enumerate(model.layers) :\n",
    "        lst_weights += [layer.get_weights()] # list of numpy arrays\n",
    "        #np.save(ary_weights,'./weight/layer{}'.format(i))\n",
    "    with open('./model_weight/lst_layer_weights_{}.h5'.format(k), \"wb\") as f:\n",
    "        pickle.dump(lst_weights,f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test : \n",
      "['A man is a a <EOS> <EOS> <EOS>', 'A monkey is pushing a <EOS> <EOS> <EOS>', 'A boy is a on a <EOS> <EOS>', 'A man is doing pull ups doorway <EOS>', 'A man is is a a <EOS> <EOS>']\n",
      "['ScdUht-pM6s_53_63.avi', 'wkgGxsuNVSg_34_41.avi', 'BtQtRGI0F2Q_15_20.avi', 'k06Ge9ANKM8_5_16.avi', 'sZf3VDsdDPM_107_114.avi']\n"
     ]
    }
   ],
   "source": [
    "### prediction\n",
    "\n",
    "### train data prediction\n",
    "if train_data_loading :\n",
    "    if teacherForce :\n",
    "        ary_pred_train = model.predict([ary_train_EC_input, ary_train_DC_input])\n",
    "    else :\n",
    "        ary_temp = np.zeros((len(ary_train_EC_input),1,lang.n_words))\n",
    "        ary_temp[:,0,0] = 1\n",
    "        ary_pred_train = model.predict([ary_train_EC_input, ary_temp])\n",
    "    ary_pred_argmax_train = np.argmax(ary_pred_train,axis=2)\n",
    "    print (ary_pred_train.shape)\n",
    "    print (ary_pred_argmax_train.shape)\n",
    "    print ('\\n')\n",
    "\n",
    "    lst_ans_string_train = ary_pred_to_sentence(ary_pred_train)\n",
    "    print ('train : ')\n",
    "    print (lst_ans_string_train[:5])\n",
    "\n",
    "### testing data prediction\n",
    "# model_test = model_DC_only1(lang)\n",
    "# ary_pred_test = model_test.predict([ary_test_EC_input, ary_test_DC_input])\n",
    "ary_pred_test = model.predict([ary_test_EC_input, ary_test_DC_input])\n",
    "\n",
    "lst_ans_string_test = ary_pred_to_sentence(ary_pred_test)\n",
    "# print ('test : ')\n",
    "# print (lst_ans_string_test[:5])\n",
    "# print (lst_id_test[:5])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         0                                    1\n",
      "0    klteYv1Uv9A_27_33.avi                         A man is a a\n",
      "1    5YJaS2Eswg0_22_26.avi           A woman is walking the the\n",
      "2  UbmZAe5u5FI_132_141.avi  A woman is seasoning meat meat meat\n",
      "3    JntMAcTlOF0_50_70.avi              Two men are dancing a a\n",
      "4  tJHUH9tpqPg_113_118.avi                       A man is a a a\n"
     ]
    }
   ],
   "source": [
    "### for special task\n",
    "if special_task :\n",
    "    lst_id_special = ['klteYv1Uv9A_27_33.avi','5YJaS2Eswg0_22_26.avi','UbmZAe5u5FI_132_141.avi','JntMAcTlOF0_50_70.avi','tJHUH9tpqPg_113_118.avi']\n",
    "    lst_ans_pair = []\n",
    "    for i,id in enumerate(lst_id_test) :\n",
    "        if id in lst_id_special :\n",
    "            ans = lst_ans_string_test[i].split(' <EOS>')[0]\n",
    "            lst_ans_pair += [[id,ans]]\n",
    "    \n",
    "    lst_ans_pair_sort = []\n",
    "\n",
    "    for id in lst_id_special :\n",
    "        for ans_pair in lst_ans_pair :\n",
    "            if id == ans_pair[0] :\n",
    "                lst_ans_pair_sort += [ans_pair]\n",
    "    df_ans = pd.DataFrame(lst_ans_pair_sort)\n",
    "    print (df_ans)\n",
    "    df_ans.to_csv(\"./{}\".format(str_output), index=False, header=False)\n",
    "   \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
