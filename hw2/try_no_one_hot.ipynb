{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import torch\n",
    "import pickle\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.layers import *\n",
    "from keras.models import *\n",
    "from keras import backend as K\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt \n",
    "def init():\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    session = tf.Session(config=config)\n",
    "    keras.backend.tensorflow_backend.set_session(session)\n",
    "init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### path and parameter\n",
    "path_data = './MLDS_hw2_data/'\n",
    "str_output = 'output_testset.txt'\n",
    "str_output_peer_review = 'output_peer_review.txt'\n",
    "\n",
    "model_name = 's2s'\n",
    "\n",
    "max_seq = 12\n",
    "n_caption = -1\n",
    "\n",
    "loading_model = 0\n",
    "do_training = 1\n",
    "teacherForce = 1\n",
    "# after_teacherForce_train = 0\n",
    "after_teacherForce_test = 0\n",
    "\n",
    "save_model = 1\n",
    "train_data_loading = 1\n",
    "test_data_loading = 1\n",
    "peer_review_data_loading = 0\n",
    "\n",
    "special_task = 0\n",
    "\n",
    "# if len(sys.argv) > 1 :\n",
    "#     path_data = sys.argv[1]\n",
    "#     str_output = sys.argv[2]\n",
    "#     str_output_peer_review = sys.argv[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lang:\n",
    "    def __init__(self):\n",
    "        self.word2index = {\"<padding>\": 0, \"<BOS>\": 1, \"<EOS>\" :2}\n",
    "        self.word2count = {\"<padding>\": 0 ,\"<BOS>\": 0, \"<EOS>\" : 0}\n",
    "        self.index2word = {0: \"<padding>\", 1: \"<BOS>\", 2: \"<EOS>\"}\n",
    "        self.n_words = 3  # Count BOS and EOS\n",
    "        self.max_len_seq = 0\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        lst_word = sentence.split()\n",
    "        if len(lst_word) > max_seq :\n",
    "            return 0\n",
    "        elif self.max_len_seq < len(lst_word) :\n",
    "            self.max_len_seq = len(lst_word)\n",
    "        for word in lst_word :\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keras_log_plot(log) :\n",
    "    matplotlib.rcParams.update({'font.size': 16})\n",
    "    fig = plt.figure(1,figsize=(20,10))\n",
    "    \n",
    "    plt.subplot(121)\n",
    "    plt.plot(log['acc'], label='train_acc')\n",
    "    plt.plot(log['val_acc'], label='val_acc')\n",
    "    plt.legend(fontsize=20)\n",
    "    plt.xlabel('epoch', fontsize=20, color='black')\n",
    "    plt.ylabel('acc', fontsize=20, color='black')\n",
    "\n",
    "    plt.subplot(122)\n",
    "    plt.plot(log['loss'], label='train_loss')\n",
    "    plt.plot(log['val_loss'], label='val_loss')\n",
    "    plt.legend(fontsize=20)\n",
    "    plt.xlabel('epoch', fontsize=20, color='black')\n",
    "    plt.ylabel('loss', fontsize=20, color='black')\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lang_init(lst_dict_label_train) :\n",
    "    lang = Lang()\n",
    "    for dict_label_train in lst_dict_label_train :\n",
    "        for sentence in dict_label_train['caption'] :\n",
    "            sentence = sentence[:-1] + ' <EOS>'\n",
    "            lang.addSentence(sentence) # remove \".\"\n",
    "    print ('lang.n_words : ' + str(lang.n_words))\n",
    "    print ('lang.max_len_seq : ' + str(lang.max_len_seq))\n",
    "    assert lang.max_len_seq == max_seq, 'error here'\n",
    "    return lang\n",
    "# assert lang.word2count['<BOS>'] == lang.word2count['<EOS>'], number of \"<BOS>\" != number of \"<EOS>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### index to one-hot\n",
    "def Str2OneHot(sentence, n_class, dict_map, max_len_seq) :\n",
    "    ### sentence to lst_index_sentence\n",
    "    sentence = sentence[:-1] + ' <EOS>'\n",
    "    lst_word = sentence.split()\n",
    "    ary_oneHot = np.zeros((max_len_seq,n_class))\n",
    "    lst_index_word = [dict_map[word] for word in lst_word]\n",
    "    ary_oneHot[range(len(lst_word)),lst_index_word] = 1\n",
    "    ary_oneHot[range(len(lst_word),len(ary_oneHot)),:] = 0.0 # others all set <EOS>\n",
    "    return ary_oneHot\n",
    "    \n",
    "### just for test\n",
    "# ary = Str2OneHot('A woman goes under a horse.', lang.n_words, lang.word2index, lang.max_len_seq)\n",
    "# print (ary.shape)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### index to one-hot\n",
    "def Str2Num(sentence, n_class, dict_map, max_len_seq) :\n",
    "    ### sentence to lst_index_sentence\n",
    "    sentence = sentence[:-1] + ' <EOS>'\n",
    "    lst_word = sentence.split()\n",
    "    ary_seq_word_num = np.zeros((max_len_seq,1))\n",
    "    lst_index_word = [dict_map[word] for word in lst_word]\n",
    "# #     ary_seq_word_num = np.asarry(lst_index_word).reshape((-1,1))\n",
    "#     ary_seq_word_num[range(len_lst_word),1] = \n",
    "#     print (ary_seq_word_num[:3])\n",
    "# #     ary_oneHot[range(len(lst_word)),lst_index_word] = 1\n",
    "# #     ary_oneHot[range(len(lst_word),len(ary_oneHot)),:] = 0.0 # others all set <EOS>\n",
    "#     return ary_seq_word_num\n",
    "    return lst_index_word\n",
    "    \n",
    "### just for test\n",
    "# ary = Str2OneHot('A woman goes under a horse.', lang.n_words, lang.word2index, lang.max_len_seq)\n",
    "# print (ary.shape)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### preprocessing\n",
    "\n",
    "### for training data\n",
    "### load lst_dict_label_train\n",
    "\n",
    "with open('{}training_label.json'.format(path_data)) as f :\n",
    "    lst_dict_label_train = json.load(f)\n",
    "print ('\\ntraining_label.json : ')\n",
    "print ('caption : \\n' + str(lst_dict_label_train[0]['caption'][:2]))\n",
    "print ('id : \\n' + str(lst_dict_label_train[0]['id']))\n",
    "\n",
    "lang = lang_init(lst_dict_label_train)\n",
    "\n",
    "if train_data_loading :\n",
    "    print ('train data is loading...')\n",
    "    ### pair caption and vedio feature\n",
    "    ### output : ary_train_EC_input\n",
    "    ###          ary_train_DC_output\n",
    "    ###          ary_train_DC_input\n",
    "    lst_id_train = [dict_label_train['id'] for dict_label_train in lst_dict_label_train]\n",
    "    lst_train_EC_input = []\n",
    "    lst_train_DC_output = []\n",
    "    for i, id in enumerate(lst_id_train) :\n",
    "        lst_npy = np.load('{}training_data/feat/{}.npy'.format(path_data, id)).tolist()\n",
    "        for caption in lst_dict_label_train[i]['caption'][:n_caption] :\n",
    "            if len(caption.split()) >= max_seq : # note >=\n",
    "                continue\n",
    "            lst_train_EC_input += [lst_npy]\n",
    "#             lst_ary_OneHot = Str2OneHot(caption, lang.n_words, lang.word2index, lang.max_len_seq).tolist()\n",
    "            lst_index_word = Str2Num(caption, lang.n_words, lang.word2index, lang.max_len_seq)\n",
    "#             lst_train_DC_output += [lst_ary_OneHot]\n",
    "            lst_train_DC_output += [lst_index_word]\n",
    "    assert len(lst_train_EC_input) == len(lst_train_DC_output), \"??\"\n",
    "    ary_train_EC_input = np.asarray(lst_train_EC_input).reshape(-1,80,4096)\n",
    "    del lst_train_EC_input\n",
    "#     ary_train_DC_output = np.asarray(lst_train_DC_output).reshape(-1,lang.max_len_seq,lang.n_words)\n",
    "#     del lst_train_DC_output\n",
    "    ary_train_DC_output = pad_sequences(lst_train_DC_output, maxlen=max_seq, dtype='int32',\n",
    "    padding='post', truncating='post', value=0.)\n",
    "\n",
    "    ### add \"<BOS>\" to ary_train_DC_input\n",
    "    ary_temp = np.zeros((len(ary_train_EC_input),1))\n",
    "    ary_temp[:,0] = lang.word2index['<BOS>']\n",
    "    if teacherForce :\n",
    "        ary_train_DC_input = np.concatenate([ary_temp,ary_train_DC_output[:,:-1]],axis=1)\n",
    "    else :\n",
    "        ary_train_DC_input = ary_temp\n",
    "\n",
    "    print ('ary_train_EC_input.shape :')\n",
    "    print (ary_train_EC_input.shape)\n",
    "    print ('ary_train_DC_output.shape :')\n",
    "    print (ary_train_DC_output.shape)\n",
    "    print ('ary_train_DC_input.shape :')\n",
    "    print (ary_train_DC_input.shape)\n",
    "\n",
    "### for testing data\n",
    "if test_data_loading :\n",
    "    print ('test data is loading...')\n",
    "    with open('{}testing_label.json'.format(path_data)) as f :\n",
    "        lst_dict_label_test = json.load(f)\n",
    "    ### pair caption and vedio feature\n",
    "    ### output : ary_test_EC_input\n",
    "    lst_id_test = [dict_label_test['id'] for dict_label_test in lst_dict_label_test]\n",
    "\n",
    "    ### just check\n",
    "    lst_id_test_2 = []\n",
    "    with open('{}{}.txt'.format(path_data, 'testing_id')) as f :\n",
    "        for line in f.readlines() :\n",
    "            lst_id_test_2 += [line.rstrip('\\n')]\n",
    "    for i in range(len(lst_id_test)) :\n",
    "        assert str(lst_id_test[i]) == str(lst_id_test_2[i]), 'error here'\n",
    "\n",
    "    lst_test_EC_input = []\n",
    "    for i, id in enumerate(lst_id_test) :\n",
    "        npy = np.load('{}testing_data/feat/{}.npy'.format(path_data, id))\n",
    "        lst_test_EC_input += [npy]\n",
    "    ary_test_EC_input = np.concatenate(lst_test_EC_input,axis=0).reshape(-1,80,4096)\n",
    "\n",
    "    ary_temp = np.zeros((len(ary_test_EC_input),1))\n",
    "    ary_temp[:,0] = lang.word2index['<BOS>']\n",
    "    ary_test_DC_input = ary_temp\n",
    "    \n",
    "    print ('ary_test_EC_input.shape :')\n",
    "    print (ary_test_EC_input.shape)\n",
    "    print ('ary_test_DC_input.shape :')\n",
    "    print (ary_test_DC_input.shape)\n",
    "    \n",
    "### for peer review\n",
    "if peer_review_data_loading :\n",
    "    print ('peer review data is loading...')\n",
    "    with open('{}peer_review_id.txt'.format(path_data),'r') as f :\n",
    "        lst_id_peer_review = f.readlines()\n",
    "        for i,id in enumerate(lst_id_peer_review) :\n",
    "            lst_id_peer_review[i] = id.rstrip('\\n')\n",
    "    \n",
    "    for i, id in enumerate(lst_id_peer_review) :\n",
    "        npy = np.load('{}peer_review/feat/{}.npy'.format(path_data, id))\n",
    "        lst_peer_review_EC_input += [npy]\n",
    "    ary_peer_review_EC_input = np.concatenate(lst_peer_review_EC_input,axis=0).reshape(-1,80,4096)\n",
    "\n",
    "    ary_temp = np.zeros((len(ary_peer_review_EC_input),1))\n",
    "    ary_temp[:,0] = lang.word2index['<BOS>']\n",
    "    ary_peer_review_DC_input = ary_temp\n",
    "\n",
    "    print ('ary_peer_review_EC_input.shape :')\n",
    "    print (ary_peer_review_EC_input.shape)\n",
    "    print ('ary_peer_review_DC_input.shape :')\n",
    "    print (ary_peer_review_DC_input.shape)\n",
    "    \n",
    "print ('---data loading finished---')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def model_pretrain(lang=lang) :\n",
    "\n",
    "#     EC_input = Input(shape=(80,4096))\n",
    "#     EC_output = GRU(32,return_state=False, return_sequences=True, activation='selu')(EC_input)\n",
    "#     EC_output, EC_output_state = GRU(32,return_state=True, activation='selu')(EC_output)\n",
    "#     DC_input = Input(shape=(None,lang.n_words))\n",
    "#     DC_input_M = Masking(mask_value=0.0)(DC_input)\n",
    "\n",
    "#     DC_gru1 = GRU(32, return_sequences=True, activation='selu')\n",
    "#     DC_time_dense = TimeDistributed(Dense(lang.n_words, activation='softmax'))\n",
    "\n",
    "#     DC_output = DC_gru1(DC_input_M, initial_state=EC_output_state)\n",
    "#     DC_output = DC_time_dense(DC_output)\n",
    "\n",
    "#     model = Model([EC_input,DC_input],DC_output)\n",
    "#     model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
    "#     print (model.summary())\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def model_DC_only1(lang=lang) :\n",
    "#     EC_input = Input(shape=(80,4096))\n",
    "#     EC_output = GRU(32,return_state=False, return_sequences=True, activation='selu')(EC_input)\n",
    "#     EC_output, EC_output_state = GRU(32,return_state=True, activation='selu')(EC_output)\n",
    "#     DC_input = Input(shape=(1,lang.n_words))\n",
    "#     DC_input_M = Masking(mask_value=0.0)(DC_input)\n",
    "\n",
    "#     DC_gru1 = GRU(32, return_sequences=True, return_state=True, activation='selu')\n",
    "#     DC_time_dense = TimeDistributed(Dense(lang.n_words, activation='softmax'))\n",
    "\n",
    "#     DC_output_state = EC_output_state\n",
    "#     DC_output = DC_input_M\n",
    "#     lst_DC_output = []\n",
    "#     for _ in range(lang.max_len_seq) :\n",
    "#         DC_output, DC_output_state = DC_gru1(DC_output, initial_state=DC_output_state)\n",
    "#         DC_output = DC_time_dense(DC_output)\n",
    "#         lst_DC_output += [DC_output]\n",
    "\n",
    "#     DC_output = Lambda(lambda x: K.concatenate(x, axis=1))(lst_DC_output)\n",
    "\n",
    "#     model = Model([EC_input,DC_input],DC_output)\n",
    "#     model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
    "#     print (model.summary())\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_pretrain(lang=lang) :\n",
    "    EC_input = Input(shape=(80,4096))\n",
    "    EC_output = Bidirectional(GRU(32,return_state=False, return_sequences=True, activation='selu'), merge_mode='concat')(EC_input)\n",
    "    EC_output = BatchNormalization()(EC_output)\n",
    "    EC_output = Bidirectional(GRU(32,return_state=False, return_sequences=True, activation='selu'), merge_mode='concat')(EC_output)\n",
    "    EC_output = BatchNormalization()(EC_output)\n",
    "    EC_output, EC_output_state = GRU(64,return_state=True, activation='selu')(EC_output)\n",
    "    EC_output_stage = BatchNormalization()(EC_output_state)\n",
    "    DC_input = Input(shape=(None,lang.n_words))\n",
    "#     DC_input_M = Masking(mask_value=0.0)(DC_input)\n",
    "\n",
    "    DC_dense_1 = TimeDistributed(Dense(32))\n",
    "    DC_gru1 = GRU(64, return_sequences=True, return_state=True, activation='selu')\n",
    "    DC_gru2 = GRU(64, return_sequences=True, return_state=True, activation='selu')\n",
    "#     DC_gru3 = GRU(64, return_sequences=True, return_state=True, activation='selu')\n",
    "    DC_dense_2 = TimeDistributed(Dense(lang.n_words, activation='softmax'))\n",
    "    Ba_output_1 = TimeDistributed(BatchNormalization())\n",
    "#     Ba_output_2 = TimeDistributed(BatchNormalization())\n",
    "#     Ba_output_3 = TimeDistributed(BatchNormalization())\n",
    "\n",
    "    DC_output_state1 = EC_output_state\n",
    "    DC_output_state2 = EC_output_state\n",
    "#     DC_output_state3 = EC_output_state\n",
    "    DC_output = DC_input\n",
    "#     lst_DC_output = []\n",
    "#     for _ in range(lang.max_len_seq) :\n",
    "    DC_output = DC_dense_1(DC_output)\n",
    "    DC_output = Ba_output_1(DC_output)\n",
    "    DC_output, DC_output_state1 = DC_gru1(DC_output, initial_state=DC_output_state1)\n",
    "    DC_output, DC_output_state2 = DC_gru2(DC_output, initial_state=DC_output_state2)\n",
    "#         DC_output = Ba_output_2(DC_output)\n",
    "#         DC_output, DC_output_state3 = DC_gru3(DC_output, initial_state=DC_output_state3)\n",
    "#         DC_output = Ba_output_3(DC_output)\n",
    "    DC_output = DC_dense_2(DC_output)\n",
    "#     lst_DC_output += [DC_output]\n",
    "\n",
    "#     DC_output = Lambda(lambda x: K.concatenate(x, axis=1))(lst_DC_output)\n",
    "\n",
    "    model = Model([EC_input,DC_input],DC_output)\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
    "    print (model.summary())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_DC_only1(lang=lang) :\n",
    "    EC_input = Input(shape=(80,4096))\n",
    "    EC_output = Bidirectional(GRU(32,return_state=False, return_sequences=True, activation='selu'), merge_mode='concat')(EC_input)\n",
    "    EC_output = BatchNormalization()(EC_output)\n",
    "    EC_output = Bidirectional(GRU(32,return_state=False, return_sequences=True, activation='selu'), merge_mode='concat')(EC_output)\n",
    "    EC_output = BatchNormalization()(EC_output)\n",
    "    EC_output, EC_output_state = GRU(64,return_state=True, activation='selu')(EC_output)\n",
    "    EC_output_stage = BatchNormalization()(EC_output_state)\n",
    "    DC_input = Input(shape=(1,lang.n_words))\n",
    "#     DC_input_M = Masking(mask_value=0.0)(DC_input)\n",
    "\n",
    "    DC_dense_1 = TimeDistributed(Dense(32))\n",
    "    DC_gru1 = GRU(64, return_sequences=True, return_state=True, activation='selu')\n",
    "    DC_gru2 = GRU(64, return_sequences=True, return_state=True, activation='selu')\n",
    "#     DC_gru3 = GRU(64, return_sequences=True, return_state=True, activation='selu')\n",
    "    DC_dense_2 = TimeDistributed(Dense(lang.n_words, activation='softmax'))\n",
    "    Ba_output_1 = TimeDistributed(BatchNormalization())\n",
    "#     Ba_output_2 = TimeDistributed(BatchNormalization())\n",
    "#     Ba_output_3 = TimeDistributed(BatchNormalization())\n",
    "\n",
    "    DC_output_state1 = EC_output_state\n",
    "    DC_output_state2 = EC_output_state\n",
    "    DC_output_state3 = EC_output_state\n",
    "    DC_output = DC_input\n",
    "    lst_DC_output = []\n",
    "    for _ in range(lang.max_len_seq) :\n",
    "        DC_output = DC_dense_1(DC_output)\n",
    "        DC_output = Ba_output_1(DC_output)\n",
    "        DC_output, DC_output_state1 = DC_gru1(DC_output, initial_state=DC_output_state1)\n",
    "        DC_output, DC_output_state2 = DC_gru2(DC_output, initial_state=DC_output_state2)\n",
    "#         DC_output = Ba_output_2(DC_output)\n",
    "#         DC_output, DC_output_state3 = DC_gru3(DC_output, initial_state=DC_output_state3)\n",
    "#         DC_output = Ba_output_3(DC_output)\n",
    "        DC_output = DC_dense_2(DC_output)\n",
    "        lst_DC_output += [DC_output]\n",
    "\n",
    "    DC_output = Lambda(lambda x: K.concatenate(x, axis=1))(lst_DC_output)\n",
    "\n",
    "    model = Model([EC_input,DC_input],DC_output)\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
    "    print (model.summary())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ary_pred_to_df_ans(ary_pred, lst_id_test) :\n",
    "    ary_pred_argmax = np.argmax(ary_pred,axis=2)\n",
    "    lst_ans_numbers = ary_pred_argmax.tolist()\n",
    "    lst_ans_string = []\n",
    "    for ans_numbers in lst_ans_numbers :\n",
    "        lst_ans_string += [' '.join([lang.index2word[ans] for ans in ans_numbers]).split(' <EOS>')[0]]\n",
    "    df_ans = pd.DataFrame([lst_id_test,lst_ans_string]).T\n",
    "    return df_ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### main\n",
    "# MCP = keras.callbacks.ModelCheckpoint('./model/{}_{}.h5'.format(model_name,k), monitor='val_acc', verbose=0, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n",
    "# ES = keras.callbacks.EarlyStopping(monitor='val_acc', min_delta=0, patience=50, verbose=0, mode='auto')\n",
    "\n",
    "if teacherForce :\n",
    "    model = model_pretrain(lang)\n",
    "else :\n",
    "    model = model_DC_only1(lang)\n",
    "    if do_training :\n",
    "        ary_temp = np.zeros((len(ary_train_EC_input),1,lang.n_words))\n",
    "        ary_temp[:,0,0] = 1\n",
    "        ary_train_DC_input = ary_temp\n",
    "    \n",
    "    \n",
    "if loading_model :\n",
    "    print ('loading and seting weight...')\n",
    "    if special_task :\n",
    "        with open('./model_weight/lst_layer_weights_special.pkl'.format(i), \"rb\") as f:\n",
    "            lst_layer_weights = pickle.load(f)\n",
    "    elif teacherForce :\n",
    "        with open('./model_weight/lst_layer_weights_0.pkl'.format(i), \"rb\") as f:\n",
    "            lst_layer_weights = pickle.load(f)\n",
    "    else :\n",
    "#         with open('./weights/lst_layer_weights_noTeacher_special.pkl'.format(i), \"rb\") as f:\n",
    "        with open('./model_weight/lst_layer_weights_0.pkl'.format(i), \"rb\") as f:\n",
    "            lst_layer_weights = pickle.load(f)\n",
    "    len_model_layer = len(model.layers)\n",
    "    for i, layer in enumerate(model.layers) :\n",
    "        if i > 11 :\n",
    "            break\n",
    "        elif i < 7 :\n",
    "            layer.trainable = False\n",
    "        print (i)\n",
    "        layer.set_weights(lst_layer_weights[i])\n",
    "\n",
    "if do_training :\n",
    "    log = model.fit([ary_train_EC_input, ary_train_DC_input],ary_train_DC_output, epochs=50, batch_size=128, validation_split=0., callbacks=[]) \n",
    "    df_log = log.history\n",
    "    #fig = keras_log_plot(df_log)\n",
    "    \n",
    "if save_model :\n",
    "    print ('saving model...')\n",
    "    if not os.path.isdir('./model_weight') :\n",
    "        os.mkdir('./model_weight')\n",
    "    k = 0\n",
    "    while 1 :\n",
    "        if os.path.isfile('./model_weight/lst_layer_weights_12seq_{}.pkl'.format(k)) :\n",
    "            k += 1\n",
    "        else :\n",
    "            break\n",
    "    lst_weights = []\n",
    "    for i, layer in enumerate(model.layers) :\n",
    "        lst_weights += [layer.get_weights()] # list of numpy arrays\n",
    "        #np.save(ary_weights,'./weight/layer{}'.format(i))\n",
    "    with open('./model_weight/lst_layer_weights_{}.pkl'.format(k), \"wb\") as f:\n",
    "        pickle.dump(lst_weights,f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### prediction\n",
    "\n",
    "### train data prediction\n",
    "if after_teacherForce_test :\n",
    "    ary_temp = np.zeros((len(ary_train_EC_input),1,lang.n_words))\n",
    "    ary_temp[:,0,0] = 1\n",
    "    ary_train_DC_input = ary_temp\n",
    "\n",
    "    if train_data_loading :\n",
    "        if teacherForce :\n",
    "            ary_pred_train = model.predict([ary_train_EC_input, ary_train_DC_input])\n",
    "        else :\n",
    "            ary_temp = np.zeros((len(ary_train_EC_input),1,lang.n_words))\n",
    "            ary_temp[:,0,0] = 1\n",
    "            ary_pred_train = model.predict([ary_train_EC_input, ary_temp])\n",
    "\n",
    "        df_ans_train = ary_pred_to_df_ans(ary_pred_train, lst_id_train)\n",
    "        print ('train : ')\n",
    "        print (df_ans_train.iloc[:5])\n",
    "\n",
    "### testing data prediction\n",
    "# model_test = model_DC_only1(lang)\n",
    "# ary_pred_test = model_test.predict([ary_test_EC_input, ary_test_DC_input])\n",
    "ary_pred_test = model.predict([ary_test_EC_input, ary_test_DC_input])\n",
    "\n",
    "df_ans = ary_pred_to_df_ans(ary_pred_test, lst_id_test)\n",
    "df_ans.to_csv(\"./{}\".format(str_output), index=False, header=False)\n",
    "print ('test : ')\n",
    "print (df_ans)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ary_sample_output_testset = np.loadtxt(open(path_data + \"sample_output_testset.txt\", \"rb\"), delimiter=\",\")\n",
    "# for i, name in enumerate(ary_sample_output_testset[:][0]) :\n",
    "#     print (name)\n",
    "\n",
    "### for special task\n",
    "if special_task :\n",
    "    lst_id_special = ['klteYv1Uv9A_27_33.avi','5YJaS2Eswg0_22_26.avi','UbmZAe5u5FI_132_141.avi','JntMAcTlOF0_50_70.avi','tJHUH9tpqPg_113_118.avi']\n",
    "    lst_ans_pair = []\n",
    "    for i,id in enumerate(lst_id_test) :\n",
    "        if id in lst_id_special :\n",
    "            ans = lst_ans_string_test[i].split(' <EOS>')[0]\n",
    "            lst_ans_pair += [[id,ans]]\n",
    "    \n",
    "    lst_ans_pair_sort = []\n",
    "\n",
    "    for id in lst_id_special :\n",
    "        for ans_pair in lst_ans_pair :\n",
    "            if id == ans_pair[0] :\n",
    "                lst_ans_pair_sort += [ans_pair]\n",
    "    df_ans = pd.DataFrame(lst_ans_pair_sort)\n",
    "    print (df_ans)\n",
    "    df_ans.to_csv(\"./{}\".format(str_output), index=False, header=False)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
